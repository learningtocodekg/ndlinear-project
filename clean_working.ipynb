{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5936ef65-02a4-44df-be12-b1334a3afc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.0\n"
     ]
    }
   ],
   "source": [
    "!python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8825114b-ec62-46f9-b296-08580086b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import MACD, ADXIndicator\n",
    "from ta.volatility import AverageTrueRange, BollingerBands\n",
    "from ta.volume import OnBalanceVolumeIndicator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db69c5bb-ba96-4768-a72c-9c107cf6caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_price_column(col):\n",
    "    \"\"\"\n",
    "    Clean a price column by removing '$' and converting to float.\n",
    "    \n",
    "    Args:\n",
    "        col (pd.Series): Column to clean\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Cleaned numeric column\n",
    "    \"\"\"\n",
    "    return col.str.replace('$', '', regex=False).astype(float)\n",
    "\n",
    "def process_stock_data(df):\n",
    "    \"\"\"\n",
    "    Process a single stock's data to create sequences and targets.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Stock data with columns ['Date', 'Close/Last', 'Volume', 'Open', 'High', 'Low']\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (sequences, y, dates)\n",
    "            - sequences: List of arrays with shape [timesteps, num_features]\n",
    "            - y: List of percent changes (clipped to [-8, 8])\n",
    "            - dates: List of dates for the last day in each sequence\n",
    "    \"\"\"\n",
    "    # Clean and convert price columns to numeric\n",
    "    price_cols = ['Close/Last', 'Open', 'High', 'Low']\n",
    "    for col in price_cols:\n",
    "        df[col] = clean_price_column(df[col])\n",
    "    df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')\n",
    "    \n",
    "    # Take from row 29 (30th oldest, 0-based index) to end\n",
    "    df = df.iloc[29:].copy()\n",
    "    \n",
    "    # Check if sufficient data is available for MACD (26 periods)\n",
    "    if len(df) < 26:\n",
    "        print(f\"Insufficient data for stock: {len(df)} rows. Skipping.\")\n",
    "        return [], [], []\n",
    "    \n",
    "    # Calculate technical indicators\n",
    "    df['RSI_7'] = RSIIndicator(df['Close/Last'], window=7).rsi()\n",
    "    macd = MACD(df['Close/Last'], window_slow=26, window_fast=12)\n",
    "    df['MACD'] = macd.macd()\n",
    "    adx_window = min(14, len(df) - 1)\n",
    "    adx = ADXIndicator(df['High'], df['Low'], df['Close/Last'], window=adx_window)\n",
    "    df['ADX_14'] = adx.adx()\n",
    "    atr = AverageTrueRange(df['High'], df['Low'], df['Close/Last'], window=14)\n",
    "    df['ATR_14'] = atr.average_true_range()\n",
    "    obv = OnBalanceVolumeIndicator(df['Close/Last'], df['Volume'])\n",
    "    df['OBV'] = obv.on_balance_volume()\n",
    "    bb = BollingerBands(df['Close/Last'], window=20)\n",
    "    df['BB_upper'] = bb.bollinger_hband()\n",
    "    df['BB_lower'] = bb.bollinger_lband()\n",
    "    \n",
    "    # Drop rows with NaN indicators\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # Skip another 20 rows if possible\n",
    "    if len(df) < 20:\n",
    "        print(f\"Insufficient data after dropping NaNs: {len(df)} rows. Skipping stock.\")\n",
    "        return [], [], []\n",
    "    df = df.iloc[20:].copy().reset_index(drop=True)\n",
    "    \n",
    "    # Define features\n",
    "    features = ['Close/Last', 'Volume', 'Open', 'High', 'Low', 'RSI_7', 'MACD', 'ADX_14', 'ATR_14', 'OBV', 'BB_upper', 'BB_lower']\n",
    "    num_features = len(features)\n",
    "    rsi_index = features.index('RSI_7')\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    y = []\n",
    "    dates = []\n",
    "    for i in range(len(df) - 20):\n",
    "        seq = df.iloc[i:i+20]\n",
    "        seq_features = seq[features].values  # Shape: [timesteps, num_features]\n",
    "        # Scale features except RSI_7 within the sequence\n",
    "        scaler = StandardScaler()\n",
    "        seq_scaled = scaler.fit_transform(seq_features)\n",
    "        seq_scaled[:, rsi_index] = seq_features[:, rsi_index]  # Preserve RSI_7\n",
    "        # Keep shape as [timesteps, num_features]\n",
    "        sequences.append(seq_scaled)\n",
    "        \n",
    "        # Calculate y: percent change\n",
    "        today_close = df.iloc[i+19]['Close/Last']\n",
    "        tomorrow_close = df.iloc[i+20]['Close/Last']\n",
    "        percent_change = 100 * (tomorrow_close - today_close) / today_close\n",
    "        # Clip percent change to [-8, 8]\n",
    "        percent_change = np.clip(percent_change, -8, 8)\n",
    "        y.append(percent_change)\n",
    "        \n",
    "        # Store date of the last day in the sequence\n",
    "        dates.append(df.iloc[i+19]['Date'])\n",
    "    \n",
    "    return sequences, y, dates\n",
    "\n",
    "def load_and_process_stock_data():\n",
    "    \"\"\"\n",
    "    Load and process stock data for multiple companies, aggregating sequences and targets.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X, y)\n",
    "            - X: Array of shape [num_sequences, timesteps, num_features]\n",
    "            - y: Array of shape [num_sequences], scaled with StandardScaler\n",
    "    \n",
    "    Saves:\n",
    "        x_and_y.pkl: Pickle file containing (X, y)\n",
    "        target_scaler.pkl: Pickle file containing the StandardScaler for y\n",
    "    \"\"\"\n",
    "    stock_files = [\n",
    "        \"amzn_data.csv\", \"nflx_data.csv\", \"tsla_data.csv\", \"aapl_data.csv\",\n",
    "        \"qcom_data.csv\", \"msft_data.csv\", \"sbux_data.csv\", \"csco_data.csv\", \"meta_data.csv\"\n",
    "    ]\n",
    "    all_data = []\n",
    "    \n",
    "    for stock_file in stock_files:\n",
    "        file_path = \"./rawData/\"+stock_file\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "        sequences, y, dates = process_stock_data(df)\n",
    "        if sequences:\n",
    "            for seq, yy, dd in zip(sequences, y, dates):\n",
    "                all_data.append((seq, yy, dd))\n",
    "        else:\n",
    "            print(f\"No sequences generated for {stock_file}\")\n",
    "    \n",
    "    if not all_data:\n",
    "        raise ValueError(\"No valid sequences generated from any stock data.\")\n",
    "    \n",
    "    # Sort by date to maintain temporal order\n",
    "    all_data.sort(key=lambda x: x[2])\n",
    "    \n",
    "    # Extract X and y\n",
    "    X = np.array([item[0] for item in all_data])  # Shape: [num_sequences, timesteps, num_features]\n",
    "    y = np.array([item[1] for item in all_data])  # Shape: [num_sequences]\n",
    "    \n",
    "    # Scale the target values\n",
    "    target_scaler = StandardScaler()\n",
    "    y = target_scaler.fit_transform(y.reshape(-1, 1)).flatten()  # Reshape for scaler, then flatten back\n",
    "    \n",
    "    # Save X, y, and target scaler\n",
    "    with open(\"x_and_y.pkl\", \"wb\") as f:\n",
    "        pickle.dump((X, y), f)\n",
    "    with open(\"target_scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(target_scaler, f)\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb6d019-f01e-430e-ad77-b472a6cb1d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insufficient data for stock: 0 rows. Skipping.\n",
      "No sequences generated for qcom_data.csv\n"
     ]
    }
   ],
   "source": [
    "X,y = load_and_process_stock_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7886a2b8-4251-4273-9d5c-52ef7ca39546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19376, 20, 12), (19376,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c28caa7-6305-48a4-9f39-ab81400b11dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)  # For NumPy (used by scikit-learn)\n",
    "torch.manual_seed(SEED)  # For PyTorch\n",
    "torch.cuda.manual_seed_all(SEED)  # For CUDA (if using GPU)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30286cba-4876-4728-9468-58ae1dfb7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with reproducible shuffling\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=SEED)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False, random_state=SEED)\n",
    "\n",
    "def make_loader(X, y, bs=32, shuffle=False):\n",
    "    ds = TensorDataset(torch.tensor(X, dtype=torch.float32).to(device),\n",
    "                       torch.tensor(y, dtype=torch.float32).to(device))\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=shuffle)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = make_loader(X_train, y_train, shuffle=True)\n",
    "val_loader = make_loader(X_val, y_val)\n",
    "test_loader = make_loader(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4f31d1b-8e48-469d-ae75-d3e3c91203ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataLoader Target Statistics:\n",
      "Number of Values: 2907\n",
      "Mean: -0.004569\n",
      "Median: -0.000535\n",
      "Standard Deviation: 0.956245\n",
      "Minimum: -3.819506\n",
      "Maximum: 3.733095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Collect all target values from test_loader\n",
    "all_targets = []\n",
    "for _, targets in test_loader:\n",
    "    all_targets.append(targets.cpu().numpy())\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "# Compute statistics\n",
    "mean = np.mean(all_targets)\n",
    "median = np.median(all_targets)\n",
    "std = np.std(all_targets)\n",
    "min_val = np.min(all_targets)\n",
    "max_val = np.max(all_targets)\n",
    "num_values = len(all_targets)\n",
    "\n",
    "# Print results\n",
    "print(\"Test DataLoader Target Statistics:\")\n",
    "print(f\"Number of Values: {num_values}\")\n",
    "print(f\"Mean: {mean:.6f}\")\n",
    "print(f\"Median: {median:.6f}\")\n",
    "print(f\"Standard Deviation: {std:.6f}\")\n",
    "print(f\"Minimum: {min_val:.6f}\")\n",
    "print(f\"Maximum: {max_val:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c89b0-b5b1-4241-9942-c05767257ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM_Linear model\n",
    "class LSTM_Linear(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model with a linear output layer.\n",
    "    \n",
    "    Args:\n",
    "        input_size: Number of input features\n",
    "        hidden_size: Number of hidden units\n",
    "        output_size: Number of output units\n",
    "        num_layers: Number of LSTM layers\n",
    "        dropout_rate: Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout_rate=0.3):\n",
    "        super(LSTM_Linear, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0).to(device)\n",
    "        self.dropout = nn.Dropout(dropout_rate).to(device)\n",
    "        self.linear = nn.Linear(hidden_size, output_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.dropout(lstm_out[:, -1, :])\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# Define LSTM_NdLinear model with multi-dimensional hidden layers\n",
    "class LSTM_NdLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model with an NdLinear output layer.\n",
    "    \n",
    "    Args:\n",
    "        input_size: Number of input features\n",
    "        hidden_size: Number of hidden units\n",
    "        output_size: Number of output units\n",
    "        nd_hidden: Hidden dimensions for NdLinear\n",
    "        num_layers: Number of LSTM layers\n",
    "        dropout_rate: Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, nd_hidden=(64, 32), num_layers=2, dropout_rate=0.3):\n",
    "        super(LSTM_NdLinear, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0).to(device)\n",
    "        self.dropout = nn.Dropout(dropout_rate).to(device)\n",
    "        self.ndlinear = NdLinear(input_dims=(hidden_size, 1), hidden_size=nd_hidden).to(device)\n",
    "        self.linear = nn.Linear(np.prod(nd_hidden), output_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_out = lstm_out[:, -1, :].unsqueeze(-1)\n",
    "        nd_out = self.ndlinear(last_out)\n",
    "        nd_out = self.dropout(nd_out.view(nd_out.size(0), -1))\n",
    "        out = self.linear(nd_out)\n",
    "        return out\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, epochs=50):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model with MSE loss.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to train\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        test_loader: Test data loader\n",
    "        criterion: Loss function (MSELoss)\n",
    "        optimizer: Optimizer\n",
    "        epochs: Number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_losses, val_losses, test_loss)\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output.squeeze(), y_batch)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output.squeeze(), y_batch)\n",
    "            test_loss += loss.item()\n",
    "        test_loss /= len(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    return train_losses, val_losses, test_loss\n",
    "\n",
    "# Initialize models\n",
    "input_size = 12  # Number of features\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "nd_hidden = (64, 32)  # Multi-dimensional hidden layer for NdLinear\n",
    "\n",
    "lstm_linear = LSTM_Linear(input_size, hidden_size, output_size).to(device)\n",
    "lstm_ndlinear = LSTM_NdLinear(input_size, hidden_size, output_size, nd_hidden).to(device)\n",
    "\n",
    "# Define loss and optimizers\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer_lstm_linear = optim.Adam(lstm_linear.parameters(), lr=0.001)\n",
    "optimizer_lstm_ndlinear = optim.Adam(lstm_ndlinear.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate LSTM_Linear\n",
    "print(\"Training LSTM_Linear...\")\n",
    "train_losses_lstm_linear, val_losses_lstm_linear, test_floss_lstm_linear = train_and_evaluate(\n",
    "    lstm_linear, train_loader, val_loader, test_loader, criterion, optimizer_lstm_linear\n",
    ")\n",
    "\n",
    "# Train and evaluate LSTM_NdLinear\n",
    "print(\"Training LSTM_NdLinear...\")\n",
    "train_losses_lstm_ndlinear, val_losses_lstm_ndlinear, test_loss_lstm_ndlinear = train_and_evaluate(\n",
    "    lstm_ndlinear, train_loader, val_loader, test_loader, criterion, optimizer_lstm_ndlinear\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458998fa-5da8-449c-89f0-21d5dd050892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN_Linear model\n",
    "class RNN_Linear(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout_rate=0.3):\n",
    "        super(RNN_Linear, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0).to(device)\n",
    "        self.dropout = nn.Dropout(dropout_rate).to(device)\n",
    "        self.linear = nn.Linear(hidden_size, output_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        out = self.dropout(rnn_out[:, -1, :])\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# Define RNN_NdLinear model with multi-dimensional hidden layers\n",
    "class RNN_NdLinear(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, nd_hidden=(64, 32), num_layers=2, dropout_rate=0.3):\n",
    "        super(RNN_NdLinear, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0).to(device)\n",
    "        self.dropout = nn.Dropout(dropout_rate).to(device)\n",
    "        self.ndlinear = NdLinear(input_dims=(hidden_size, 1), hidden_size=nd_hidden).to(device)\n",
    "        self.linear = nn.Linear(np.prod(nd_hidden), output_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        last_out = rnn_out[:, -1, :].unsqueeze(-1)\n",
    "        nd_out = self.ndlinear(last_out)\n",
    "        nd_out = self.dropout(nd_out.view(nd_out.size(0), -1))\n",
    "        out = self.linear(nd_out)\n",
    "        return out\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, epochs=50):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output.squeeze(), y_batch)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output.squeeze(), y_batch)\n",
    "            test_loss += loss.item()\n",
    "        test_loss /= len(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    return train_losses, val_losses, test_loss\n",
    "\n",
    "# Initialize models\n",
    "input_size = 12  # Number of features\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "nd_hidden = (64, 32)  # Multi-dimensional hidden layer for NdLinear\n",
    "\n",
    "rnn_linear = RNN_Linear(input_size, hidden_size, output_size).to(device)\n",
    "rnn_ndlinear = RNN_NdLinear(input_size, hidden_size, output_size, nd_hidden).to(device)\n",
    "\n",
    "# Define loss and optimizers\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer_rnn_linear = optim.Adam(rnn_linear.parameters(), lr=0.001)\n",
    "optimizer_rnn_ndlinear = optim.Adam(rnn_ndlinear.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate RNN_Linear\n",
    "print(\"Training RNN_Linear...\")\n",
    "train_losses_rnn_linear, val_losses_rnn_linear, test_loss_rnn_linear = train_and_evaluate(\n",
    "    rnn_linear, train_loader, val_loader, test_loader, criterion, optimizer_rnn_linear\n",
    ")\n",
    "\n",
    "# Train and evaluate RNN_NdLinear\n",
    "print(\"Training RNN_NdLinear...\")\n",
    "train_losses_rnn_ndlinear, val_losses_rnn_ndlinear, test_loss_rnn_ndlinear = train_and_evaluate(\n",
    "    rnn_ndlinear, train_loader, val_loader, test_loader, criterion, optimizer_rnn_ndlinear\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd39d83-483f-4958-9210-b49a53ce978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TCN block\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, kernel_size=3):\n",
    "        super(TCN, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv1d(input_size, hidden_size, kernel_size=kernel_size, padding=padding).to(device)\n",
    "        self.relu = nn.ReLU().to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # [batch, features, timesteps]\n",
    "        out = self.conv(x)\n",
    "        out = self.relu(out)\n",
    "        out = out.transpose(1, 2)  # [batch, timesteps, hidden_size]\n",
    "        return out\n",
    "\n",
    "# Define TCN_Linear model\n",
    "class TCN_Linear(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.3):\n",
    "        super(TCN_Linear, self).__init__()\n",
    "        self.tcn = TCN(input_size, hidden_size).to(device)\n",
    "        self.dropout = nn.Dropout(dropout_rate).to(device)\n",
    "        self.linear = nn.Linear(hidden_size, output_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tcn_out = self.tcn(x)\n",
    "        out = self.dropout(tcn_out[:, -1, :])\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# Define TCN_NdLinear model with multi-dimensional hidden layers\n",
    "class TCN_NdLinear(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, nd_hidden=(64, 32), dropout_rate=0.3):\n",
    "        super(TCN_NdLinear, self).__init__()\n",
    "        self.tcn = TCN(input_size, hidden_size).to(device)\n",
    "        self.dropout = nn.Dropout(dropout_rate).to(device)\n",
    "        self.ndlinear = NdLinear(input_dims=(hidden_size, 1), hidden_size=nd_hidden).to(device)\n",
    "        self.linear = nn.Linear(np.prod(nd_hidden), output_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tcn_out = self.tcn(x)\n",
    "        last_out = tcn_out[:, -1, :].unsqueeze(-1)\n",
    "        nd_out = self.ndlinear(last_out)\n",
    "        nd_out = self.dropout(nd_out.view(nd_out.size(0), -1))\n",
    "        out = self.linear(nd_out)\n",
    "        return out\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, epochs=50):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output.squeeze(), y_batch)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output.squeeze(), y_batch)\n",
    "            test_loss += loss.item()\n",
    "        test_loss /= len(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    return train_losses, val_losses, test_loss\n",
    "\n",
    "# Initialize models\n",
    "input_size = 12  # Number of features\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "nd_hidden = (64, 32)  # Multi-dimensional hidden layer for NdLinear\n",
    "\n",
    "tcn_linear = TCN_Linear(input_size, hidden_size, output_size).to(device)\n",
    "tcn_ndlinear = TCN_NdLinear(input_size, hidden_size, output_size, nd_hidden).to(device)\n",
    "\n",
    "# Define loss and optimizers\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer_tcn_linear = optim.Adam(tcn_linear.parameters(), lr=0.001)\n",
    "optimizer_tcn_ndlinear = optim.Adam(tcn_ndlinear.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate TCN_Linear\n",
    "print(\"Training TCN_Linear...\")\n",
    "train_losses_tcn_linear, val_losses_tcn_linear, test_loss_tcn_linear = train_and_evaluate(\n",
    "    tcn_linear, train_loader, val_loader, test_loader, criterion, optimizer_tcn_linear\n",
    ")\n",
    "\n",
    "# Train and evaluate TCN_NdLinear\n",
    "print(\"Training TCN_NdLinear...\")\n",
    "train_losses_tcn_ndlinear, val_losses_tcn_ndlinear, test_loss_tcn_ndlinear = train_and_evaluate(\n",
    "    tcn_ndlinear, train_loader, val_loader, test_loader, criterion, optimizer_tcn_ndlinear\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420e46f-3e07-4230-875a-a5f294ee8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NdLinear-based Model\n",
    "class NdFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    NdLinear-based feedforward network for transformer, adapted from ts_forecast.py.\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Input dimension\n",
    "        hidden_dim: Multi-dimensional hidden size for NdLinear (e.g., (256, 1))\n",
    "        dropout: Dropout rate\n",
    "        activation: Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=(128, 128), dropout=0.1, activation='gelu'):\n",
    "        super(NdFeedForward, self).__init__()\n",
    "        activations = {\n",
    "            'relu': nn.ReLU().to(device),\n",
    "            'tanh': nn.Tanh().to(device),\n",
    "            'sigmoid': nn.Sigmoid().to(device),\n",
    "            'gelu': nn.GELU().to(device),\n",
    "        }\n",
    "        if activation not in activations:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        self.layer1 = NdLinear((input_dim, 1), hidden_dim).to(device)\n",
    "        self.activation = activations[activation]\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "        self.layer2 = NdLinear(hidden_dim, (input_dim, 1)).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_dims = list(x.shape)\n",
    "        x = x.reshape(x_dims[0] * x_dims[1], x_dims[2], 1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x_dims[0], x_dims[1], x_dims[2])\n",
    "        return x\n",
    "\n",
    "class NdTransformerEncoderLayer(nn.TransformerEncoderLayer):\n",
    "    \"\"\"\n",
    "    Transformer encoder layer with NdLinear feedforward, adapted from ts_forecast.py.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        nhead: Number of attention heads\n",
    "        custom_ffn: NdFeedForward instance\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead, custom_ffn, **kwargs):\n",
    "        kwargs['batch_first'] = True\n",
    "        super(NdTransformerEncoderLayer, self).__init__(d_model, nhead, **kwargs)\n",
    "        self.nd_ffn = custom_ffn.to(device)\n",
    "\n",
    "    def _ff_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.nd_ffn(x)\n",
    "        return x\n",
    "\n",
    "class Transformer_NdLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model with NdLinear feedforward for stock price prediction.\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Number of input features (12)\n",
    "        model_dim: Transformer model dimension\n",
    "        num_heads: Number of attention heads\n",
    "        num_layers: Number of transformer layers\n",
    "        hidden_dim: Hidden dimension for NdFeedForward\n",
    "        dropout: Dropout rate\n",
    "        activation: Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, model_dim=64, num_heads=2, num_layers=3, hidden_dim=(128, 128), dropout=0.1, activation='gelu'):\n",
    "        super(Transformer_NdLinear, self).__init__()\n",
    "        ff_layer = NdFeedForward(model_dim, hidden_dim=hidden_dim, dropout=dropout, activation=activation).to(device)\n",
    "        encoder_layer = NdTransformerEncoderLayer(model_dim, num_heads, custom_ffn=ff_layer, dropout=dropout).to(device)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(device)\n",
    "        self.embedding = nn.Linear(input_dim, model_dim).to(device)\n",
    "        self.fc_out = nn.Linear(model_dim, 1).to(device)\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output[:, -1, :]  # Take the last timestep\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Standard Linear-based Model\n",
    "class LinearFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard feedforward network with Linear layers.\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Input dimension\n",
    "        hidden_dim: Hidden dimension for Linear layers\n",
    "        dropout: Dropout rate\n",
    "        activation: Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.1, activation='gelu'):\n",
    "        super(LinearFeedForward, self).__init__()\n",
    "        activations = {\n",
    "            'relu': nn.ReLU().to(device),\n",
    "            'tanh': nn.Tanh().to(device),\n",
    "            'sigmoid': nn.Sigmoid().to(device),\n",
    "            'gelu': nn.GELU().to(device),\n",
    "        }\n",
    "        if activation not in activations:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim).to(device)\n",
    "        self.activation = activations[activation]\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "        self.layer2 = nn.Linear(hidden_dim, input_dim).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "class Transformer_Linear(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model with standard Linear feedforward for stock price prediction.\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Number of input features (12)\n",
    "        model_dim: Transformer model dimension\n",
    "        num_heads: Number of attention heads\n",
    "        num_layers: Number of transformer layers\n",
    "        hidden_dim: Hidden dimension for FeedForward\n",
    "        dropout: Dropout rate\n",
    "        activation: Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, model_dim=64, num_heads=2, num_layers=3, hidden_dim=256, dropout=0.1, activation='gelu'):\n",
    "        super(Transformer_Linear, self).__init__()\n",
    "        ff_layer = LinearFeedForward(model_dim, hidden_dim=hidden_dim, dropout=dropout, activation=activation).to(device)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(model_dim, num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True).to(device)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(device)\n",
    "        self.embedding = nn.Linear(input_dim, model_dim).to(device)\n",
    "        self.fc_out = nn.Linear(model_dim, 1).to(device)\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output[:, -1, :]  # Take the last timestep\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, epochs=50, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Train and evaluate the model with MSE loss and mixed precision.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model (NdLinear or Linear)\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        test_loader: Test data loader\n",
    "        criterion: Loss function (MSELoss)\n",
    "        optimizer: Optimizer\n",
    "        epochs: Number of training epochs\n",
    "        model_name: Name of the model for logging\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_losses, val_losses, test_loss)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets in tqdm(train_loader, desc=f'Training {model_name} Epoch {epoch+1}/{epochs}', leave=False):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "                val_loss += loss.item()\n",
    "                val_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            val_loss /= len(val_loader)\n",
    "            val_outputs = np.concatenate(val_outputs)\n",
    "            output_std = np.std(val_outputs)\n",
    "            print(f\"{model_name} Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Output Std: {output_std:.6f}, Min Val Pred: {val_outputs.min():.6f}, Max Val Pred: {val_outputs.max():.6f}\")\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_outputs = []\n",
    "    test_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            test_loss += loss.item()\n",
    "            test_outputs.append(outputs.squeeze().cpu().numpy())\n",
    "            test_targets.append(targets.cpu().numpy())\n",
    "        test_loss /= len(test_loader)\n",
    "        test_outputs = np.concatenate(test_outputs)\n",
    "        test_targets = np.concatenate(test_targets)\n",
    "        print(f\"{model_name} Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"{model_name} Test Outputs: Min: {test_outputs.min():.6f}, Max: {test_outputs.max():.6f}, Std: {np.std(test_outputs):.6f}\")\n",
    "        print(f\"{model_name} Test Targets: Min: {test_targets.min():.6f}, Max: {test_targets.max():.6f}, Std: {np.std(test_targets):.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses, test_loss\n",
    "\n",
    "# Initialize and train NdLinear model\n",
    "input_dim = 12  # Number of features\n",
    "model_ndlinear = Transformer_NdLinear(input_dim, model_dim=64, num_heads=2, num_layers=3, hidden_dim=(128,128), dropout=0.1, activation='gelu').to(device)\n",
    "optimizer_ndlinear = optim.Adam(model_ndlinear.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "print(\"Training Transformer_NdLinear...\")\n",
    "train_losses_ndlinear, val_losses_ndlinear, test_loss_ndlinear = train_and_evaluate(\n",
    "    model_ndlinear, train_loader, val_loader, test_loader, criterion, optimizer_ndlinear, epochs=50, model_name=\"Transformer_NdLinear\"\n",
    ")\n",
    "\n",
    "# Save NdLinear model\n",
    "torch.save(model_ndlinear.state_dict(), 'transformer_ndlinear.pth')\n",
    "\n",
    "# Initialize and train Linear model\n",
    "model_linear = Transformer_Linear(input_dim, model_dim=64, num_heads=2, num_layers=3, hidden_dim=256, dropout=0.1, activation='gelu').to(device)\n",
    "optimizer_linear = optim.Adam(model_linear.parameters(), lr=0.0001)\n",
    "print(\"Training Transformer_Linear...\")\n",
    "train_losses_linear, val_losses_linear, test_loss_linear = train_and_evaluate(\n",
    "    model_linear, train_loader, val_loader, test_loader, criterion, optimizer_linear, epochs=50, model_name=\"Transformer_Linear\"\n",
    ")\n",
    "\n",
    "# Save Linear model\n",
    "torch.save(model_linear.state_dict(), 'transformer_linear.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcebe1-f0e1-4b14-8509-be4ebeda2988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_trading(model, dataloader, target_scaler_path=\"./rawData/target_scaler.pkl\", initial_cash=10000, transaction_cost=0.001):\n",
    "    \"\"\"\n",
    "    Simulate trading with scaled allocation for positive predictions and inverse positions for negative predictions.\n",
    "    For negative predictions, sells long position before buying inverse position. Handles scaled targets by descaling.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model (LSTM_Linear, LSTM_NdLinear, RNN, TCN, or Transformer_NdLinear)\n",
    "        dataloader: PyTorch DataLoader with input data [batch_size, timesteps, num_features] and scaled targets [batch_size]\n",
    "        target_scaler_path: Path to the saved StandardScaler for targets (default: 'target_scaler.pkl')\n",
    "        initial_cash: Starting capital (default: $10,000)\n",
    "        transaction_cost: Cost per trade as a fraction (default: 0.1%)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains portfolio_values, returns, and sharpe_ratio\n",
    "    \"\"\"\n",
    "    # Load the target scaler\n",
    "    with open(target_scaler_path, \"rb\") as f:\n",
    "        target_scaler = pickle.load(f)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    cash = initial_cash\n",
    "    position = 0  # Number of units held (positive for long or inverse positions)\n",
    "    is_inverse = False  # Flag to track if position is inverse\n",
    "    portfolio_values = [initial_cash]\n",
    "    returns = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # inputs: [batch_size, timesteps, num_features], targets: [batch_size]\n",
    "            \n",
    "            # Get model predictions for the batch\n",
    "            preds = model(inputs).squeeze()  # Shape: [batch_size]\n",
    "            \n",
    "            # Inverse-transform predictions and targets to percent changes\n",
    "            preds_np = preds.cpu().numpy().reshape(-1, 1)\n",
    "            targets_np = targets.cpu().numpy().reshape(-1, 1)\n",
    "            preds_percent = target_scaler.inverse_transform(preds_np).flatten()  # Shape: [batch_size]\n",
    "            targets_percent = target_scaler.inverse_transform(targets_np).flatten()  # Shape: [batch_size]\n",
    "            \n",
    "            # Process each sample in the batch\n",
    "            for pred, actual_pct in zip(preds_percent, targets_percent):\n",
    "                # Trading logic\n",
    "                allocation = 0.0\n",
    "                if pred >= 1:\n",
    "                    # Positive predictions: Linear scaling between 1% and 2%\n",
    "                    abs_pred = abs(pred)\n",
    "                    if abs_pred >= 2:\n",
    "                        allocation = 1.0  # 100% allocation for predictions >= 2%\n",
    "                    else:\n",
    "                        allocation = 0.5 * (abs_pred - 1) + 0.5  # 1% -> 50%, 2% -> 100%\n",
    "                    target_is_inverse = False\n",
    "                elif pred < 0:\n",
    "                    # Negative predictions: First sell any existing position\n",
    "                    if position > 0:\n",
    "                        if is_inverse:\n",
    "                            # Sell inverse position\n",
    "                            position_value = position / (1 + actual_pct / 100)  # Inverse position gains when price falls\n",
    "                            cash += position_value * (1 - transaction_cost)\n",
    "                        else:\n",
    "                            # Sell long position\n",
    "                            cash += position * (1 + actual_pct / 100) * (1 - transaction_cost)\n",
    "                        position = 0\n",
    "                        is_inverse = False\n",
    "                    \n",
    "                    # Then check for inverse position if prediction is below -0.5%\n",
    "                    if pred < -0.5:\n",
    "                        abs_pred = abs(pred)\n",
    "                        if abs_pred >= 1.5:\n",
    "                            allocation = 1.0  # 100% allocation for predictions <= -1.5%\n",
    "                        elif abs_pred >= 0.5:\n",
    "                            # Linear scaling: -0.5% -> 0%, -1% -> 50%, -1.5% -> 100%\n",
    "                            allocation = 0.5 * (abs_pred - 0.5) / (1.5 - 0.5)\n",
    "                        target_is_inverse = True\n",
    "                    else:\n",
    "                        target_is_inverse = False\n",
    "                else:\n",
    "                    target_is_inverse = False\n",
    "                \n",
    "                if pred > 0 and cash > 0 and allocation > 0:\n",
    "                    # Buy long position: Invest allocation fraction of cash\n",
    "                    invest_amount = cash * allocation\n",
    "                    shares_to_buy = invest_amount / (1 + actual_pct / 100) / (1 + transaction_cost)\n",
    "                    position = shares_to_buy\n",
    "                    cash -= invest_amount\n",
    "                    is_inverse = False\n",
    "                elif pred < -0.5 and cash > 0 and allocation > 0:\n",
    "                    # Buy inverse position: Invest allocation fraction of cash\n",
    "                    invest_amount = cash * allocation\n",
    "                    units_to_buy = invest_amount * (1 + actual_pct / 100) / (1 + transaction_cost)  # Inverse units scale with price\n",
    "                    position = units_to_buy\n",
    "                    cash -= invest_amount\n",
    "                    is_inverse = True\n",
    "                \n",
    "                # Update portfolio value\n",
    "                if is_inverse:\n",
    "                    # Inverse position: Gains when price falls\n",
    "                    position_value = position / (1 + actual_pct / 100)\n",
    "                else:\n",
    "                    # Long position: Gains when price rises\n",
    "                    position_value = position * (1 + actual_pct / 100)\n",
    "                portfolio_value = cash + position_value\n",
    "                # Prevent negative portfolio value\n",
    "                portfolio_value = max(0, portfolio_value)\n",
    "                portfolio_values.append(portfolio_value)\n",
    "                \n",
    "                # Calculate daily return\n",
    "                daily_return = (portfolio_value - portfolio_values[-2]) / portfolio_values[-2]\n",
    "                returns.append(daily_return)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    returns = np.array(returns)\n",
    "    total_return = (portfolio_values[-1] - initial_cash) / initial_cash\n",
    "    sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252) if np.std(returns) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'portfolio_values': portfolio_values,\n",
    "        'total_return': total_return,\n",
    "        'sharpe_ratio': sharpe_ratio\n",
    "    }\n",
    "\n",
    "# Example usage with trained models\n",
    "# Assume lstm_linear, lstm_ndlinear, rnn_linear, rnn_ndlinear, tcn_linear, tcn_ndlinear, model_ndlinear, model_linear are trained\n",
    "# Use test_loader created from X_test and y_test (scaled targets)\n",
    "results = {}\n",
    "\n",
    "# Simulate trading for LSTM_Linear\n",
    "print(\"Simulating trading for LSTM_Linear...\")\n",
    "results['LSTM_Linear'] = simulate_trading(lstm_linear.to(device), test_loader)\n",
    "\n",
    "# Simulate trading for LSTM_NdLinear\n",
    "print(\"Simulating trading for LSTM_NdLinear...\")\n",
    "results['LSTM_NdLinear'] = simulate_trading(lstm_ndlinear.to(device), test_loader)\n",
    "\n",
    "# Simulate trading for RNN_Linear\n",
    "print(\"Simulating trading for RNN_Linear...\")\n",
    "results['RNN_Linear'] = simulate_trading(rnn_linear.to(device), test_loader)\n",
    "\n",
    "# Simulate trading for RNN_NdLinear\n",
    "print(\"Simulating trading for RNN_NdLinear...\")\n",
    "results['RNN_NdLinear'] = simulate_trading(rnn_ndlinear.to(device), test_loader)\n",
    "\n",
    "# Simulate trading for TCN_Linear\n",
    "print(\"Simulating trading for TCN_Linear...\")\n",
    "results['TCN_Linear'] = simulate_trading(tcn_linear.to(device), test_loader)\n",
    "\n",
    "# Simulate trading for TCN_NdLinear\n",
    "print(\"Simulating trading for TCN_NdLinear...\")\n",
    "results['TCN_NdLinear'] = simulate_trading(tcn_ndlinear.to(device), test_loader)\n",
    "\n",
    "# Simulate trading for Transformer_NdLinear\n",
    "print(\"Simulating trading for Transformer_NdLinear...\")\n",
    "results['Transformer_NdLinear'] = simulate_trading(model_ndlinear.to(device), test_loader)\n",
    "\n",
    "# Simulate trading for Transformer_Linear\n",
    "print(\"Simulating trading for Transformer_Linear...\")\n",
    "results['Transformer_Linear'] = simulate_trading(model_linear.to(device), test_loader)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name, res in results.items():\n",
    "    plt.plot(res['portfolio_values'], label=f\"{name} (Return: {res['total_return']:.2%}, Sharpe: {res['sharpe_ratio']:.2f})\")\n",
    "plt.title(\"Portfolio Value Over Time (Stock Trading)\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Portfolio Value ($)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('trading_results.png')\n",
    "plt.close()\n",
    "# Print summary\n",
    "for name, res in results.items():\n",
    "    print(f\"{name}: Final Value: ${res['portfolio_values'][-1]:.2f}, Total Return: {res['total_return']:.2%}, Sharpe Ratio: {res['sharpe_ratio']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb4c61-eb03-43d4-9fc5-0b15d6116afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020aa928-0a26-49b1-9a13-607f9a283b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281f1ae-6fc8-42a6-ad87-9250406d5cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e6b69-1476-4a78-b4d4-f13586e7ded9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fcd32c-41ff-494d-b2b1-f51ebd354906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6ab0c-a164-4911-bade-295d85d79e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a79833-5e6b-487d-a4e9-a99b73d8505a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78217c32-29af-4d2e-b5af-7c1ef92cea8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785097a-87b9-4159-8819-4cfa4dcadb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9306eae-cbd5-4e92-b306-24466c809477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
